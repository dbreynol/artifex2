# Time Series {#ts}

## Example

Suppose we have data on global mean land temperature deviations (from 1951-1980 average). Let's see what this data looks like. 

```{r}
plot(globtempl, main = "Global mean land (only) temperature deviations")
```

How could we model this response. Given we only have one tool in our tool box, let's use it and fit a simple linear regression model using time as the predictor.


```{r}
data = data.frame(y = as.numeric(globtempl), x = as.numeric(time(globtempl)))

m0 = lm(y ~ x, data = data)
betas = coefficients(m0)

#summary(m0)

with(data, plot(x, y, main = "Regression of temperature deviations versus time"))
abline(betas[1], betas[2])
```

We can look at the residuals from this model as a quick diagnostic.

```{r}
plot(y = residuals(m0), x = as.numeric(time(globtempl)),
     main = "Residuals of Simple Linear Regression")
abline(a=0, b=0)
```

Perhaps we should use a time series model instead in which current values of the series are modelled as a function of past values. That is,

\begin{equation}
y_t = \delta + \phi y_{t-1} + \epsilon_t.
(\#eq:armod)
\end{equation}

In order to satisfy the assumptions of such a model, however, we need our time series to be (weakly) stationary. A series  is said to be (weakly) stationary if it satisfies the following properties:

- The mean  is the same for all $y_t$.
- The variance of  is the same for all $y_t$.
- The covariance (and also correlation) between  and  is the same for all  at each lag  $h = 1, 2, 3$, etc.

Does our time series satisfy this condition? Maybe if we do a transformation, we can satisfy the condition. Let's take the <code>diff</code>. That is, let,

\begin{equation}
x_t = y_t -  y_{t-1}.
\end{equation}

Let's examine a plot of these differences.


```{r}
y = diff(globtempl)
plot(y, main = "Differences in temperature deviations")
```

This looks decently stationary. Let's fit a model of the form \@ref(eq:armod) to the differences data. This can be achieved with the <code>arima</code> function.

```{r echo=TRUE}
m0 = arima(diff(globtempl), order = c(1,0,0))
#summary(m0)
```

Let's look at the predicted versus actual values from this model.

```{r}
fx = y - m0$residuals

fx.orig = globtempl + fx

g2 = window(globtempl, start = 1881)
plot(fx.orig, col = "red", ylab = "temp deviations", main = "Predicted (red) versus actual (blue) values")
lines(g2, col = "blue")
```

This certainly looks like an improvement over linear regression!

## Autocorrelation

A central concept in time series analysis is autocorrelation. This is the correlation between $y_t$ and its lagged value. For a lag of $h$, this is the correlation between $y_t$ and $y_{t-h}$. For the differenced series above, this can be found using the <code>acf</code> function.

```{r}
#acf(y)
```


## Exercises 7

Write your own program to compute (part 1) and plot (**optional part 2**) the autocorrelation function of a given time series, for a specified number of lags.

The autocorrelation for a time series $y$ at lag $k$ is:

\begin{equation}
r_k = \frac{ \sum_{t = k + 1}^{T} (y_t - \bar{y})(y_{t-k} - \bar{y}) }{\sum_{t=1}^T (y_t - \bar{y})^2 }
\end{equation}


As an example, the code below generates the lag 1 autocorrelation for a time series $y$:

```{r echo = T}
# simulate time series data from an AR(1) model
y = arima.sim(n=10, list(ar = c(.9), sd = 0.5))

# split into two vectors; one that starts one lag value late (late1)
# and one that ends one lag value early (early1)
late1 = as.numeric( window(y, start = 2))
early1 = as.numeric ( window(y, end = 9) )

ybar = mean(as.numeric(y))

sum ( (late1 - ybar ) * (early1 - ybar ) ) /  ( sum ( (y - ybar)^2 ) )

# check using builtin function: (acf(y, type = "correlation"))
```

For your function, accept two arguments: the lag value and the time series. Output the acf for the given lag. Once you complete this, optionally, use that function in a new function to plot the first 5 $r_k$ values using barplots. Compare with <code>acf</code>.


## Teams Feedback 1

**Team Shrek**

1. Look at a decomposed version of your response variable. Figure out what the function <code>decompose</code> is doing. See the code below for cardiovascular mortality.

```{r echo = T}
plot(lap[,3], main = "Cardiovascular Mortality")
decomp = decompose(lap[,3])
plot(decomp)
```

2. Figure out what is driving the seasonality in mortality in LA. Do some research.

3. Are there any outlier weeks in your data? What is causing them?

4. Recall that standard time series models require stationary data (constant mean and variance over time). Inspect the seasonally differenced data ($x_t = y_t - y_{t-52}$) to see if it is stationary. See code below.

```{r echo = T}
diff_52 = diff(lap[,3],  lag = 52)
plot(diff_52)
```


5. A potentially appropriate model for seasonal data like this is a seasonal AR model. The partial autocorrelation function is a way to select which terms to include in the model. See below.

```{r echo=TRUE}
acf(diff_52, type = "partial", lag.max = 104) # what terms to include
# mod = sarima(lap[,3], 2, 0, 0, 1, 1, 0, S = 52)
# sarima.for(lap[,3], 52, 2, 0, 0, 1, 1, 0, 52)
```

6. Examine scatterplots of your response variable and your potential predictors? Only use predictors if you have a reason for them (they may reasonably influence the response).

**Team NBA 3**

1. Convert your response and predictor variables into time series objects using <code>as.ts</code>, a function in the **astsa** package. Examine a decomposed version of each variable. Do these reveal anything interesting?

2. See if you can attain a stationary time series for your response using a simple transformation (hint: <code>diff</code>). Once you obtain a stationary series, look at the partial autocorrelation of the series (hint: <code>acf( , type = "partial")</code>). Fit at AR model based on this plot.

3. Examine scatterplots of your (stationary) response versus potential predictor variables. For example ($y_t - y_{t-1}$ versus $x_{t-1}$). Which predictors look like they are associated with the response?

4. Based on 3, fit a linear regression and examine residuals as a function of time. Is there a trend?

**Team Beast**

1. Make scatterplots (in R) of your response versus each predictor. What are the relationships you observe?

2. Fit a (multivariable) linear regression model to the (subset of) the data you deem relevant. See example below.

```{r echo=TRUE}

# model hwy MPG (hwy) as a function of engine displacement (displ) and year of mfg (year)

lm(hwy ~ displ + year, data = mpg) %>% tidy() %>% kable()

```

3. Brainstorm ways to make this model better. In what way is it defificent? How you can address this?


## Team Shrek

Find a stationary version of your response variable. For example,  is the series $x_t = y_t - y_{t-52}$ stationary? Let's plot it.

```{r}
diff_52 = diff(lap[,1],  lag = 52)
plot(diff_52)
```

Ok, so let's fit a model in which $x_t$ is the response and we use past values of the response to predict the current value. This is called a seasonal autoregressive model. Which past values should we include? To figure this out, let's use a partial autocorrelation plot and see which values exceed the dotted lines.

```{r echo=TRUE}
acf(diff_52, type = "partial", lag.max = 104) # what terms to include
```

It looks like the last two periods ($x_{t-1}, x_{t-2}$) have significant correlation with the response, as well as the one and two year ago values, $x_{t-52}, x_{t-104}$. We can fit this model using the <code>sarima</code> function as,

```{r message=F, warning=FALSE, echo = T}
mod1 = sarima(lap[,1], 2, 0, 0, 2, 1, 0, S = 52)
```

You can also see forecasts from this model, using <code>sarima.for</code>:

```{r echo = T}
sarima.for(lap[,1], 52, 2, 0, 0, 2, 1, 0, S = 52)
```

How about adding in external regressors? You can do this using the <code>xreg</code> argument within <code>sarima</code>. For instance, a model that includes <code>part</code> as a regressor is:

```{r echo = T, message=F, warning=FALSE}
mod1 = sarima(lap[,1], 2, 0, 0, 2, 1, 0, S = 52, xreg = lap[,11])
```

**Questions**

1. How much better is the model (AIC) when the regressor is included?
2. Forecast new values using the model with the regressor. You will need predicted values of the regressor. There are many ways to do this (such as using a past year's value or fitting a model to the regressor itself using the steps above).
3. What are your conclusions?


## Team NBA

1. Data input and proceessing

```{r echo = T}
nba.dat = read.csv('Team_stats_per_game.csv')


# filter to start in 1980 (when 3 pointers started)
# and create a response variable - 3 point attempts, averaged across teams
# and predictor - mean 3 point conversion pct.
nba.processed = nba.dat %>% 
  filter(season > 1979) %>%
  group_by(season) %>%
  summarise(y = mean(x3pa_per_game), x = mean(x3p_percent))

```

2. Make the response into a time series object and examine the autocorrelation. Lags with significant values in the plot should be included in an autoregressive model. This plot seems to suggest an order one AR model with the lag one differences as the response.

```{r echo = T}
# make the response into a time series object - which clearly has an increasing trend
# inspect the lag 1 differences for stationarity - looks good
yt = as.ts(nba.processed$y, start = 1980, end = 2022)
plot(diff(yt))
acf(yt, type = "partial") # order one AR model seems appropriate
```

3. Let's fit the model suggested above and examine what it predicts for the next five years.

```{r echo = T}
m0 = arima(yt, order = c(1, 1, 0)) # fit AR model with one past period to the differenced data
predict(m0, n.ahead = 5) # predict 5 years into the future
```

4. Now let's add a predictor variable, the 3 point conversion rate. First, let's make it into a time series object and then include as a predictor in the AR model. We can also use it to make predictions.

```{r echo = T}
xt = as.ts(nba.processed$x, start = 1980, end = 2022)
m1 = arima(yt, order = c(1, 1, 0), xreg = xt)
predict(m1, n.ahead = 5, newxreg = window(xt, start = 39))
```

**Questions**

1. How much better is the model (AIC) when the regressor is included?
2. What are your interpretations of the model? What are the limitations?
3. Other options could include adding other predictors, different model type, expanding on EDA (can you learn anything from team level trends?)


